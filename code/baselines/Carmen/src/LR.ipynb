{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import jaccard_score\n",
    "import os\n",
    "import argparse\n",
    "from util import Metrics\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util import multi_label_metric\n",
    "model_name = 'LR'\n",
    "resume_path = 'saved/{}/Epoch_49_JA_0.4603_DDI_0.07427.model'.format(model_name)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--Test', action='store_true', default=False, help=\"test mode\")\n",
    "parser.add_argument('--FT', action='store_true', default=False, help=\"Fine Tune\")\n",
    "parser.add_argument('--datadir', type=str, default=\"../data/\", help='dimension')\n",
    "parser.add_argument('--ftfile', type=str, default=\"emm\", help='finetune file')\n",
    "parser.add_argument('--cuda', type=int, default=-1, help='use cuda')\n",
    "parser.add_argument('--epoch', type=int, default=400, help='# of epoches')\n",
    "parser.add_argument('--early_stop', type=int, default=30, help='early stop number')\n",
    "parser.add_argument('--resume_path', type=str, default=resume_path, help='resume path')\n",
    "parser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\n",
    "parser.add_argument('--model_name', type=str, default=model_name, help=\"model name\")\n",
    "parser.add_argument('--seed', type=int, default=1029, help='use cuda')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.MIMIC=3\n",
    "if not os.path.exists(os.path.join(\"saved\", args.model_name)):\n",
    "    os.makedirs(os.path.join(\"saved\", args.model_name))\n",
    "def create_dataset(data, diag_voc, pro_voc, med_voc):\n",
    "    i1_len = len(diag_voc.idx2word)\n",
    "    i2_len = len(pro_voc.idx2word)\n",
    "    output_len = len(med_voc.idx2word)\n",
    "    input_len = i1_len + i2_len\n",
    "    X = []\n",
    "    y = []\n",
    "    for patient in data:\n",
    "        for visit in patient:\n",
    "            i1 = visit[0]\n",
    "            i2 = visit[1]\n",
    "            o = visit[2]\n",
    "\n",
    "            multi_hot_input = np.zeros(input_len)\n",
    "            multi_hot_input[i1] = 1\n",
    "            multi_hot_input[np.array(i2) + i1_len] = 1\n",
    "\n",
    "            multi_hot_output = np.zeros(output_len)\n",
    "            multi_hot_output[o] = 1\n",
    "\n",
    "            X.append(multi_hot_input)\n",
    "            y.append(multi_hot_output)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# grid_search = False\n",
    "# data_path = os.path.join(args.datadir, 'records_final_4.pkl')\n",
    "# voc_path = os.path.join(args.datadir, 'voc_final_4.pkl')\n",
    "data_path = os.path.join(args.datadir, 'records_final.pkl')\n",
    "voc_path = os.path.join(args.datadir, 'voc_final.pkl')\n",
    "\n",
    "data = dill.load(open(data_path, 'rb'))\n",
    "voc = dill.load(open(voc_path, 'rb'))\n",
    "diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n",
    "metric_obj = Metrics(data, med_voc, args)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "thre idx: 57\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "for epoch in range(1):\n",
    "\n",
    "    np.random.seed(args.seed)\n",
    "    np.random.shuffle(data)\n",
    "    split_point = int(len(data) * 2 / 3)\n",
    "    data_train = data[:split_point]\n",
    "    eval_len = int(len(data[split_point:]) / 2)\n",
    "    data_eval = data[split_point+eval_len:]\n",
    "    data_test = data[split_point:split_point + eval_len]\n",
    "    print('1')\n",
    "    train_X, train_y = create_dataset(data_train, diag_voc, pro_voc, med_voc)\n",
    "    test_X, test_y = create_dataset(data_test, diag_voc, pro_voc, med_voc)\n",
    "    eval_X, eval_y = create_dataset(data_eval, diag_voc, pro_voc, med_voc)\n",
    "    model = LogisticRegression()\n",
    "    classifier = OneVsRestClassifier(model)\n",
    "    print('2')\n",
    "\n",
    "    tic = time.time()\n",
    "    classifier.fit(train_X, train_y)\n",
    "    print('3')\n",
    "\n",
    "    fittime = time.time() - tic\n",
    "    print ('fitting time: {}'.format(fittime))\n",
    "\n",
    "\n",
    "    result = []\n",
    "    for _ in range(1):\n",
    "        # index = np.random.choice(np.arange(len(test_X)), round(len(test_X) * 0.8), replace=True)\n",
    "        test_sample = test_X  # [index]\n",
    "        y_sample = test_y  # [index]\n",
    "        y_pred = classifier.predict(test_sample)\n",
    "        pretime = time.time() - tic\n",
    "        print ('inference time: {}'.format(pretime))\n",
    "\n",
    "        y_prob = classifier.predict_proba(test_sample)\n",
    "\n",
    "        metric_obj.set_data(y_sample, y_pred, y_prob, save=args.Test)\n",
    "        ja, prauc, avg_p, avg_r, avg_f1 = metric_obj.run()\n",
    "        # ja, prauc, avg_p, avg_r, avg_f1 = multi_label_metric(y_sample, y_pred, y_prob)\n",
    "\n",
    "        # ddi rate\n",
    "        ddi_adj_path = os.path.join(args.datadir, 'ddi_A_final.pkl')\n",
    "        # ddi_adj_path = os.path.join(args.datadir, 'ddi_A_final_4.pkl')\n",
    "        ddi_A = dill.load(open(ddi_adj_path, 'rb'))\n",
    "        all_cnt = 0\n",
    "        dd_cnt = 0\n",
    "        med_cnt = 0\n",
    "        visit_cnt = 0\n",
    "        for adm in y_pred:\n",
    "            med_code_set = np.where(adm==1)[0]\n",
    "            visit_cnt += 1\n",
    "            med_cnt += len(med_code_set)\n",
    "            for i, med_i in enumerate(med_code_set):\n",
    "                for j, med_j in enumerate(med_code_set):\n",
    "                    if j <= i:\n",
    "                        continue\n",
    "                    all_cnt += 1\n",
    "                    if ddi_A[med_i, med_j] == 1 or ddi_A[med_j, med_i] == 1:\n",
    "                        dd_cnt += 1\n",
    "        ddi_rate = dd_cnt / all_cnt\n",
    "        result.append([ddi_rate, ja, avg_f1, prauc, med_cnt / visit_cnt])\n",
    "    \n",
    "    result = np.array(result)\n",
    "    mean = result.mean(axis=0)\n",
    "    std = result.std(axis=0)\n",
    "\n",
    "    outstring = \"\"\n",
    "    for m, s in zip(mean, std):\n",
    "        outstring += \"{:.4f} $\\pm$ {:.4f} & \".format(m, s)\n",
    "\n",
    "    print (outstring)\n",
    "\n",
    "    tic = time.time()\n",
    "    print('Epoch: {}, DDI Rate: {:.4}, Jaccard: {:.4}, PRAUC: {:.4}, AVG_PRC: {:.4}, AVG_RECALL: {:.4}, AVG_F1: {:.4}, AVG_MED: {:.4}\\n'.format(\n",
    "        epoch, ddi_rate, ja, prauc, avg_p, avg_r, avg_f1, med_cnt / visit_cnt\n",
    "        ))\n",
    "\n",
    "    history = defaultdict(list)\n",
    "    history['fittime'].append(fittime)\n",
    "    history['pretime'].append(pretime)\n",
    "    history['jaccard'].append(ja)\n",
    "    history['ddi_rate'].append(ddi_rate)\n",
    "    history['avg_p'].append(avg_p)\n",
    "    history['avg_r'].append(avg_r)\n",
    "    history['avg_f1'].append(avg_f1)\n",
    "    history['prauc'].append(prauc)\n",
    "\n",
    "dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n",
    "print('Avg_Fittime: {:.8}, Avg_Pretime: {:.8}, Avg_Jaccard: {:.4}, Avg_DDI: {:.4}, Avg_p: {:.4}, Avg_r: {:.4}, \\\n",
    "        Avg_f1: {:.4}, AVG_PRC: {:.4}\\n'.format(\n",
    "    np.mean(history['fittime']),\n",
    "    np.mean(history['pretime']),\n",
    "    np.mean(history['jaccard']),\n",
    "    np.mean(history['ddi_rate']),\n",
    "    np.mean(history['avg_p']),\n",
    "    np.mean(history['avg_r']),\n",
    "    np.mean(history['avg_f1']),\n",
    "    np.mean(history['prauc'])\n",
    "    ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "fitting time: 128.0398235321045\n",
      "inference time: 129.1986653804779\n",
      "adverse DDI number: -1.0000, Jaccard: 0.4936,  PRAUC: 0.7578, AVG_PRC: 0.7283, AVG_RECALL: 0.6109, AVG_F1: 0.6503, AVG_MED: 16.1644\n",
      "\n",
      "0.0838 $\\pm$ 0.0000 & 0.4936 $\\pm$ 0.0000 & 0.6503 $\\pm$ 0.0000 & 0.7578 $\\pm$ 0.0000 & 16.1644 $\\pm$ 0.0000 & \n",
      "Epoch: 0, DDI Rate: 0.08378, Jaccard: 0.4936, PRAUC: 0.7578, AVG_PRC: 0.7283, AVG_RECALL: 0.6109, AVG_F1: 0.6503, AVG_MED: 16.16\n",
      "\n",
      "Avg_Fittime: 128.03982, Avg_Pretime: 129.19867, Avg_Jaccard: 0.4936, Avg_DDI: 0.08378, Avg_p: 0.7283, Avg_r: 0.6109,         Avg_f1: 0.6503, AVG_PRC: 0.7578\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "result"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.08378316,  0.49357555,  0.65027099,  0.75781962, 16.16435644]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "y_sample"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "ja"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4935755455881565"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import utils_"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils_'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\Github Repos\\MyNet\\final2\\code\\baselines\\Carmen\\src\\LR.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mutils_\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils_'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "sys.path.append('../..')\n",
    "import utils_"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils_'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\Github Repos\\MyNet\\final2\\code\\baselines\\Carmen\\src\\LR.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m../..\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mutils_\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils_'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def multi_label_metric2(y_gt, y_pred, y_prob):\n",
    "    # Jaccard系数\n",
    "    def jaccard(y_gt, y_pred):\n",
    "        score = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            target = np.where(y_gt[b] == 1)[0]\n",
    "            out_list = np.where(y_pred[b] == 1)[0]\n",
    "            inter = set(out_list) & set(target)\n",
    "            union = set(out_list) | set(target)\n",
    "            jaccard_score = 0 if len(union)==0 else len(inter) / len(union)\n",
    "            score.append(jaccard_score)\n",
    "        return np.mean(score)\n",
    "    # 平均精确率\n",
    "    def average_prc(y_gt, y_pred):\n",
    "        score = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            target = np.where(y_gt[b] == 1)[0]\n",
    "            out_list = np.where(y_pred[b] == 1)[0]\n",
    "            inter = set(out_list) & set(target)\n",
    "            prc_score = 0 if len(out_list) == 0 else len(inter) / len(out_list)\n",
    "            score.append(prc_score)\n",
    "        return score\n",
    "    # 平均召回率\n",
    "    def average_recall(y_gt, y_pred):\n",
    "        score = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            target = np.where(y_gt[b] == 1)[0]\n",
    "            out_list = np.where(y_pred[b] == 1)[0]\n",
    "            inter = set(out_list) & set(target)\n",
    "            recall_score = 0 if len(target) == 0 else len(inter) / len(target)\n",
    "            score.append(recall_score)\n",
    "        return score\n",
    "    # 平均F1\n",
    "    def average_f1(average_prc, average_recall):\n",
    "        score = []\n",
    "        for idx in range(len(average_prc)):\n",
    "            if average_prc[idx] + average_recall[idx] == 0:\n",
    "                score.append(0)\n",
    "            else:\n",
    "                score.append(2*average_prc[idx]*average_recall[idx] / (average_prc[idx] + average_recall[idx]))\n",
    "        return score\n",
    "    # Macro F1\n",
    "    def f1(y_gt, y_pred):\n",
    "        all_micro = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            all_micro.append(f1_score(y_gt[b], y_pred[b], average='macro'))\n",
    "        return np.mean(all_micro)\n",
    "    # 平均精确率\n",
    "    def precision_auc(y_gt, y_prob):\n",
    "        all_micro = []\n",
    "        for b in range(len(y_gt)):\n",
    "            all_micro.append(average_precision_score(y_gt[b], y_prob[b], average='macro'))\n",
    "        return np.mean(all_micro)\n",
    "    # Macro F1\n",
    "    f1 = f1(y_gt, y_pred)\n",
    "    # 平均精确率\n",
    "    prauc = precision_auc(y_gt, y_prob)\n",
    "    # Jaccard系数\n",
    "    ja = jaccard(y_gt, y_pred)\n",
    "    # Precision, Recall, F1\n",
    "    avg_prc = average_prc(y_gt, y_pred)\n",
    "    avg_recall = average_recall(y_gt, y_pred)\n",
    "    avg_f1 = average_f1(avg_prc, avg_recall)\n",
    "    return ja, prauc, np.mean(avg_prc), np.mean(avg_recall), np.mean(avg_f1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "multi_label_metric2(test_y, y_pred, y_prob)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\Desktop\\Github Repos\\MyNet\\final2\\code\\baselines\\Carmen\\src\\LR.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#%%\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m multi_label_metric2(test_y, y_pred, y_prob)\n",
      "Cell \u001b[1;32mIn[9], line 56\u001b[0m, in \u001b[0;36mmulti_label_metric2\u001b[1;34m(y_gt, y_pred, y_prob)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(all_micro)\n\u001b[0;32m     55\u001b[0m \u001b[39m# Macro F1\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m f1 \u001b[39m=\u001b[39m f1(y_gt, y_pred)\n\u001b[0;32m     57\u001b[0m \u001b[39m# 平均精确率\u001b[39;00m\n\u001b[0;32m     58\u001b[0m prauc \u001b[39m=\u001b[39m precision_auc(y_gt, y_prob)\n",
      "Cell \u001b[1;32mIn[9], line 47\u001b[0m, in \u001b[0;36mmulti_label_metric2.<locals>.f1\u001b[1;34m(y_gt, y_pred)\u001b[0m\n\u001b[0;32m     45\u001b[0m all_micro \u001b[39m=\u001b[39m []\n\u001b[0;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(y_gt\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m---> 47\u001b[0m     all_micro\u001b[39m.\u001b[39mappend(f1_score(y_gt[b], y_pred[b], average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(all_micro)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from sklearn.metrics import f1_score, average_precision_score;\n",
    "def multi_label_metric2(y_gt, y_pred, y_prob):\n",
    "    # Jaccard系数\n",
    "    def jaccard(y_gt, y_pred):\n",
    "        score = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            target = np.where(y_gt[b] == 1)[0]\n",
    "            out_list = np.where(y_pred[b] == 1)[0]\n",
    "            inter = set(out_list) & set(target)\n",
    "            union = set(out_list) | set(target)\n",
    "            jaccard_score = 0 if len(union)==0 else len(inter) / len(union)\n",
    "            score.append(jaccard_score)\n",
    "        return np.mean(score)\n",
    "    # 平均精确率\n",
    "    def average_prc(y_gt, y_pred):\n",
    "        score = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            target = np.where(y_gt[b] == 1)[0]\n",
    "            out_list = np.where(y_pred[b] == 1)[0]\n",
    "            inter = set(out_list) & set(target)\n",
    "            prc_score = 0 if len(out_list) == 0 else len(inter) / len(out_list)\n",
    "            score.append(prc_score)\n",
    "        return score\n",
    "    # 平均召回率\n",
    "    def average_recall(y_gt, y_pred):\n",
    "        score = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            target = np.where(y_gt[b] == 1)[0]\n",
    "            out_list = np.where(y_pred[b] == 1)[0]\n",
    "            inter = set(out_list) & set(target)\n",
    "            recall_score = 0 if len(target) == 0 else len(inter) / len(target)\n",
    "            score.append(recall_score)\n",
    "        return score\n",
    "    # 平均F1\n",
    "    def average_f1(average_prc, average_recall):\n",
    "        score = []\n",
    "        for idx in range(len(average_prc)):\n",
    "            if average_prc[idx] + average_recall[idx] == 0:\n",
    "                score.append(0)\n",
    "            else:\n",
    "                score.append(2*average_prc[idx]*average_recall[idx] / (average_prc[idx] + average_recall[idx]))\n",
    "        return score\n",
    "    # Macro F1\n",
    "    def f1(y_gt, y_pred):\n",
    "        all_micro = []\n",
    "        for b in range(y_gt.shape[0]):\n",
    "            all_micro.append(f1_score(y_gt[b], y_pred[b], average='macro'))\n",
    "        return np.mean(all_micro)\n",
    "    # 平均精确率\n",
    "    def precision_auc(y_gt, y_prob):\n",
    "        all_micro = []\n",
    "        for b in range(len(y_gt)):\n",
    "            all_micro.append(average_precision_score(y_gt[b], y_prob[b], average='macro'))\n",
    "        return np.mean(all_micro)\n",
    "    # Macro F1\n",
    "    f1 = f1(y_gt, y_pred)\n",
    "    # 平均精确率\n",
    "    prauc = precision_auc(y_gt, y_prob)\n",
    "    # Jaccard系数\n",
    "    ja = jaccard(y_gt, y_pred)\n",
    "    # Precision, Recall, F1\n",
    "    avg_prc = average_prc(y_gt, y_pred)\n",
    "    avg_recall = average_recall(y_gt, y_pred)\n",
    "    avg_f1 = average_f1(avg_prc, avg_recall)\n",
    "    return ja, prauc, np.mean(avg_prc), np.mean(avg_recall), np.mean(avg_f1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "multi_label_metric2(test_y, y_pred, y_prob)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.4935755455881565,\n",
       " 0.7578196215931153,\n",
       " 0.7282814174760223,\n",
       " 0.6108980584707616,\n",
       " 0.6502709857843286)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "ja"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4935755455881565"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "prauc"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7578196215931153"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "avg_p"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7282814174760223"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "avg_f1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6502709857843286"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}